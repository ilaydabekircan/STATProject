---
title: "Untitled"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(ggplot2)
```

```{r}
data.bank <- read.csv("/Users/victoriaa/Desktop/STAT5405/DATA/BankChurners.csv")
data.bank
```

```{r}
summary(data.bank)
```

```{r}
sum(is.na(data.bank))

```

```{r}
# Dropping the CLIENTNUM column using Base R
data.bank <- data.bank[, -which(names(data.bank) == "CLIENTNUM")]
# Dropping the last two columns
data.bank <- data.bank[, -c((ncol(data.bank)-1), ncol(data.bank))]
summary(data.bank)
```

```{r}
(correlation_matrix <- cor(data.bank[sapply(data.bank, is.numeric)]))
```

```{r}
#correlation between Credit_Limit and Avg_Open_To_Buy
print(correlation_matrix[9, "Credit_Limit"])
```

Extremely high correlation between predictors can cause multicollinearity, making it challenging for the regression model to estimate unique coefficients for each predictor. Also, multicollinearity can lead to inflated standard errors for the coefficients, making the estimation less precise. Therefore, we drop the column 'Avg_Open_To_Buy' to reduce instability.

**Note to Victoria: If we don't drop this column, logit model gives na values for Avg_Open_To_Buy and vif cannot be calculated, it gives an error saying that there is a perfect correlation between two predictors.**

```{r}
data.bank <- data.bank[, -which(names(data.bank) == "Avg_Open_To_Buy")]
```

```{r}
data.bank$Attrition_Flag <- as.factor(ifelse(data.bank$Attrition_Flag == "Existing Customer", 0, 1))
table(data.bank$Attrition_Flag)
```

```{r}

data.bank$Gender <- as.factor(data.bank$Gender)
data.bank$Education_Level <- as.factor(data.bank$Education_Level)
data.bank$Marital_Status <- as.factor(data.bank$Marital_Status)
data.bank$Income_Category <- as.factor(data.bank$Income_Category)
data.bank$Card_Category <- as.factor(data.bank$Card_Category)

str(data.bank)

```

```{r}

# Bar chart comparing the counts of Education Level within each Marital Status category
ggplot(data.bank, aes(x = Education_Level, fill = Marital_Status)) +
  geom_bar(position = "dodge") +
  labs(x = "Education Level", y = "Count", fill = "Marital Status") +
  theme_minimal()

```

```{r}
# Boxplot comparing Customer Age across different Education Levels
ggplot(data.bank, aes(x = Education_Level, y = Customer_Age, fill = Education_Level)) +
  geom_boxplot() +
  labs(x = "Education Level", y = "Customer Age") +
  theme_minimal()

```

```{r}
ggplot(data.bank, aes(x = Income_Category, y = Customer_Age, fill = Income_Category)) +
  geom_boxplot() +
  labs(x = "Income Category", y = "Customer Age") +
  theme_minimal()
```

```{r}
ggplot(data.bank, aes(x = Customer_Age)) +
  geom_histogram(bins = 30, fill = "blue", color = "white") +
  labs(x = "Customer Age", y = "Frequency") +
  theme_minimal()
```

```{r}
ggplot(data.bank, aes(x = Credit_Limit, fill = Gender)) +
  geom_density(alpha = 0.7) +
  labs(x = "Credit Limit", y = "Density", fill = "Gender") +
  theme_minimal()
```

```{r}
ggplot(data.bank, aes(x = Attrition_Flag)) +
  geom_bar(aes(fill = Attrition_Flag)) +
  labs(x = "Attrition Flag", y = "Count", fill = "Attrition Status") +
  theme_minimal()
```

```{r}
# Scatter Plot of Total Transactions vs. Credit Limit
ggplot(data.bank, aes(x = Total_Trans_Amt, y = Credit_Limit, color = Attrition_Flag)) +
  geom_point(alpha = 0.6) +
  labs(x = "Total Transactions Amount", y = "Credit Limit", color = "Attrition Flag") +
  theme_minimal()
```

```{r}
# setting seed before splitting train and test sets
set.seed(42)
ratio <- 0.80
train_set <- sort(sample(1:nrow(data.bank), ceiling(nrow(data.bank)*ratio)))

# creating train and test set
data.train <- data.bank[train_set, ]
data.test  <- data.bank[-train_set, ]
```

```{r}
# 0 and 1 ratio of data, train and test
(summary(data.bank$Attrition_Flag) / nrow(data.bank))
(summary(data.train$Attrition_Flag) / nrow(data.train))
(summary(data.test$Attrition_Flag) / nrow(data.test))
```

```{r}
logit.null <- glm(Attrition_Flag ~ 1, 
                  data = data.train, 
                  family = binomial(link = "logit"))
summary(logit.null)
```

```{r}
library(caret)
```

```{r}
pred.logit_null <- predict(logit.null, newdata = data.test, type="response")

condition.logit_null <- ifelse(pred.logit_null > 0.5, 1, 0)
(confusion_matrix.logit_null <- confusionMatrix(reference = as.factor(data.test$Attrition_Flag), 
                                                data = as.factor(condition.logit_null), 
                                                positive = "1",
                                                mode="everything"))
```

```{r}
library(pROC)
auc(data.test$Attrition_Flag, pred.logit_null)
```

```{r}
logit.full <- glm(Attrition_Flag ~ ., 
                  data = data.train, 
                  family = binomial(link = "logit"))
summary(logit.full)
```

```{r}
pred.logit_full <- predict(logit.full, newdata = data.test, type="response")

condition.logit_full <- ifelse(pred.logit_full > 0.5, 1, 0)
(confusion_matrix.logit_full <- confusionMatrix(reference = as.factor(data.test$Attrition_Flag), 
                                                data = as.factor(condition.logit_full), 
                                                positive = "1",
                                                mode="everything"))
```

```{r}
auc(data.test$Attrition_Flag, pred.logit_full)
```

```{r}
car::vif(logit.full)
```

```{r}
logit.both <- step(logit.null, list(lower=formula(logit.null),
                                    upper=formula(logit.full)),
                                    direction="both",
                                    trace=0, 
                                    data = data.train)
summary(logit.both)
```

```{r}
pred.logit_both <- predict(logit.both, newdata = data.test, type="response")

condition.logit_both <- ifelse(pred.logit_both > 0.5, 1, 0)
(confusion_matrix.logit_both <- confusionMatrix(reference = as.factor(data.test$Attrition_Flag), 
                                                data = as.factor(condition.logit_both), 
                                                positive = "1",
                                                mode="everything"))
```

```{r}
auc(data.test$Attrition_Flag, pred.logit_both)
```

| Model Metrics     | Full Logit Model | Reduced Logit Model |
|-------------------|------------------|---------------------|
| Residual Deviance | 3737.3           | 3747.8              |
| AIC               | 3801.3           | 3793.8              |
| Accuracy          | 89.73%           | 89.73%              |
| Sensitivity       | 96.17%           | 95.99%              |
| Specificity       | 56.67%           | 57.58%              |
| AUC               | 0.9219           | 0.9215              |

-   Add deviance residual plots for null, full and reduced logit
-   Outliers, leverage points, cooks distance, dffits

**Residual Diagnostic Plots**

```{r}
par(mfrow=c(2,2))
plot(logit.full)
```

```{r}
outliers <- which(abs(residuals(logit.full)) > 3*sd(residuals(logit.full)))

```

```{r}
par(mfrow=c(2,2))
plot(logit.full.2)
```

**Checking for high leverage cases**

```{r}
n <- nrow(data.train.2)
p <- ncol(data.train.2)-1
chilev <- which(influence(logit.full)$hat > max(2*(p+1)/n, 0.5))
chilev
```

**Checking for high influential points**

```{r}
cooksD <- cooks.distance(logit.full)
highCooksD <- which(cooksD > (4/(n-p-1)))
highCooksD
```

```{r}
data.train.3 <- data.train.2[-highCooksD,]
logit.full.2 <- glm(Attrition_Flag ~ ., 
                  data = data.train.3, 
                  family = binomial(link = "logit"))
summary(logit.full.2)

```

```{r}

pred.logit_full.2 <- predict(logit.full.2, newdata = data.test, type="response")

condition.logit_full.2 <- ifelse(pred.logit_full.2 > 0.5, 1, 0)
(confusion_matrix.logit_full.2 <- confusionMatrix(reference = as.factor(data.test$Attrition_Flag), 
                                                data = as.factor(condition.logit_full.2), 
                                                positive = "1",
                                                mode="everything"))
```

**explore interaction or transformation**

### Decision Tree

```{r}
library(rpart)
```

```{r}
fit.allpred <- rpart(Attrition_Flag ~., method = "class", data = data.train,
                  control = rpart.control(minsplit = 1, cp = 0.001))
```

```{r}
printcp(fit.allpred) 
```

```{r}
(cp= fit.allpred$cptable[which.min(fit.allpred$cptable[, "xerror"]), "CP"])
(xerr = fit.allpred$cptable[which.min(fit.allpred$cptable[, "xerror"]), "xerror"])
```

```{r}
plotcp(fit.allpred)
```

```{r}
library(rpart.plot)
```

```{r}
rpart.plot(fit.allp, extra = "auto")
```

```{r}
test_df <- data.frame(actual = data.test$Attrition_Flag, pred = NA)
test_df$pred <- predict(fit.allpred, newdata = data.test, type = "class")
# Create the confusion matrix using caret
conf_matrix <- confusionMatrix(as.factor(test_df$pred), as.factor(test_df$actual), positive = "1")
conf_matrix
```

**Pruning Tree**

```{r}

prunefit.allp <- prune(fit.allpred, cp =
    fit.allpred$cptable[which.min(fit.allpred$cptable[, "xerror"]), "CP"])
```

```{r}
rpart.plot(prunefit.allp, extra = "auto")
```

```{r}
#summary(prunefit.allp)
```

The error rate computed on the training sample is

```{r}
rootnode_err <- sum(data.train$Attrition_Flag==1)/nrow(data.train)
prelerr = prunefit.allp$cptable[which.min(prunefit.allp$cptable[, "rel error"]), "rel error"]
(presub.err_rate <- rootnode_err*prelerr) 
```

error rate computed on the test sample is

```{r}
rootnode_err <- sum(data.train$Attrition_Flag==1)/nrow(data.train)
pxerr = prunefit.allp$cptable[which.min(prunefit.allp$cptable[, "xerror"]), "xerror"]
(pcv.err_rate <- rootnode_err*pxerr)
```

```{r}
test_df <- data.frame(actual = data.test$Attrition_Flag, pred = NA)
test_df$prediction <- predict(prunefit.allp, newdata = data.test, type = "class")
conf_matrix_pruned_tree <- confusionMatrix(as.factor(test_df$prediction), as.factor(test_df$actual), positive = "1")
conf_matrix_pruned_tree
```

```{r}
# Missclassification error rate:
misclassification_error <- sum(conf_matrix_pruned_tree$table[2,1], conf_matrix_pruned_tree$table[1,2]) /
                          sum(conf_matrix_pruned_tree$table)
misclassification_error

```

## Random Forest

```{r}
library(ranger)
```

```{r}
fit.rf <- ranger(Attrition_Flag ~ ., data = data.train, 
                   importance = 'impurity', mtry = 3)
print(fit.rf)
```

```{r}
library(vip)
```

```{r}
(v1 <- vi(fit.rf))
```

```{r}
vip(v1)
```

```{r}
pred <- predict(fit.rf, data = data.test)
test_df <- data.frame(actual = data.test$Attrition_Flag, pred = NA)
test_df$predictions <- pred$predictions
conf_matrix_rf <- confusionMatrix(as.factor(test_df$predictions), as.factor(test_df$actual), positive = "1")
conf_matrix_rf
```

```{r}
library(caret)

recall <- sensitivity(conf_matrix_rf$table, positive = "1")
precision <- precision(conf_matrix_rf$table, positive = "1")
f1_score <- F_meas(conf_matrix_rf$table, positive = "1")

# Print the metrics
print(paste("Recall:", recall))
print(paste("Precision:", precision))
print(paste("F1 Score:", f1_score))
  
```

## Gradient Boosting

```{r}
library(xgboost)
library(Matrix)
```

```{r}
# Transform the predictor matrix using dummy (or indicator or one-hot) encoding 
matrix_predictors.train <- 
  as.matrix(sparse.model.matrix(Attrition_Flag ~., data = data.train))[, -1]
matrix_predictors.test <- 
  as.matrix(sparse.model.matrix(Attrition_Flag ~., data = data.test))[, -1]
```

```{r}
# Train dataset
pred.train.gbm <- data.matrix(matrix_predictors.train) # predictors only
#convert factor to numeric
data.train.gbm <- as.numeric(as.character(data.train$Attrition_Flag)) 
dtrain <- xgb.DMatrix(data = pred.train.gbm, label = data.train.gbm)
# Test dataset
pred.test.gbm <- data.matrix(matrix_predictors.test) # predictors only
 #convert factor to numeric
data.test.gbm <- as.numeric(as.character(data.test$Attrition_Flag))
dtest <- xgb.DMatrix(data = pred.test.gbm, label = data.test.gbm)
```

```{r}
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
              objective = "binary:logistic", eval_metric = "auc")
```

```{r}
model.xgb <- xgb.train(param, dtrain, nrounds = 7, watchlist)
```

```{r}
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(data.train.gbm, prediction.train))
```

```{r}
sum(diag(tab))/sum(tab)
```

```{r}
threshold <- 0.5 # This threshold can be adjusted
prediction <- as.numeric(pred.y > threshold) # Convert probabilities to binary predictions

# Now, create the test data frame
test_df <- data.frame(actual = data.test$Attrition_Flag, prediction = prediction)

# Convert both actual and predicted to factors assuming '1' is the positive class
test_df$actual <- as.factor(test_df$actual)
test_df$prediction <- as.factor(test_df$prediction)

# Recreate the confusion matrix
xg.conf_matrix <- confusionMatrix(test_df$prediction, test_df$actual, positive = "1")
print(xg.conf_matrix)

```

```{r}
recall <- sensitivity(xg.conf_matrix$table, positive = "1")

# Calculate precision (positive predictive value)
precision <- posPredValue(xg.conf_matrix$table, positive = "1")

# Calculate F1 score
f1_score <- (2 * precision * recall) / (precision + recall)

# Print the metrics
print(paste("Recall:", recall))
print(paste("Precision:", precision))
print(paste("F1 Score:", f1_score))
```
